{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Linear Regression\n",
    "**DS4400 - Machine Learning 1**  \n",
    "**Vignan Kamarthi**  \n",
    "**Due: 2/13/2026 at 11pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "- Drop columns: `id`, `date`, `zipcode`\n",
    "- Scale features to mean 0 and std 1\n",
    "- Divide price by 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1000 samples, 18 features\n",
      "Testing set: 1000 samples, 18 features\n",
      "Features: ['Unnamed: 0', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop excluded columns\n",
    "drop_cols = ['id', 'date', 'zipcode']\n",
    "train_df = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "test_df = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
    "\n",
    "# Divide price by 1000\n",
    "train_df['price'] = train_df['price'] / 1000\n",
    "test_df['price'] = test_df['price'] / 1000\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'price'\n",
    "feature_cols = [c for c in train_df.columns if c != target_col]\n",
    "\n",
    "X_train_raw = train_df[feature_cols].values\n",
    "y_train = train_df[target_col].values\n",
    "X_test_raw = test_df[feature_cols].values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "# Standardize features (fit on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features')\n",
    "print(f'Testing set: {X_test.shape[0]} samples, {X_test.shape[1]} features')\n",
    "print(f'Features: {feature_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Linear Regression with Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Train and report coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 520.414834000001\n",
      "\n",
      "Coefficients:\n",
      "      Feature  Coefficient\n",
      "   Unnamed: 0     8.456024\n",
      "     bedrooms   -12.807339\n",
      "    bathrooms    18.456913\n",
      "  sqft_living    57.161582\n",
      "     sqft_lot    11.127338\n",
      "       floors     8.151038\n",
      "   waterfront    64.230911\n",
      "         view    47.610288\n",
      "    condition    12.647609\n",
      "        grade    92.511076\n",
      "   sqft_above    48.439051\n",
      "sqft_basement    27.688812\n",
      "     yr_built   -68.043173\n",
      " yr_renovated    17.341926\n",
      "          lat    78.129852\n",
      "         long    -1.437669\n",
      "sqft_living15    45.479128\n",
      "   sqft_lot15   -12.906560\n",
      "\n",
      "Training MSE: 31415.7479\n",
      "Training R^2: 0.7271\n"
     ]
    }
   ],
   "source": [
    "# Problem 2, Part 1: Train sklearn LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Report coefficients\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': lr_model.coef_\n",
    "})\n",
    "print(\"Intercept:\", lr_model.intercept_)\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coeff_df.to_string(index=False))\n",
    "\n",
    "# Training metrics\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTraining MSE: {train_mse:.4f}\")\n",
    "print(f\"Training R^2: {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Evaluate on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MSE:  58834.6740\n",
      "Testing R^2:  0.6471\n",
      "\n",
      "--- Summary ---\n",
      "Metric            Train       Test\n",
      "MSE          31415.7479 58834.6740\n",
      "R^2              0.7271     0.6471\n"
     ]
    }
   ],
   "source": [
    "# Problem 2, Part 2: Evaluate on testing set\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Testing MSE:  {test_mse:.4f}\")\n",
    "print(f\"Testing R^2:  {test_r2:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"{'Metric':<12} {'Train':>10} {'Test':>10}\")\n",
    "print(f\"{'MSE':<12} {train_mse:>10.4f} {test_mse:>10.4f}\")\n",
    "print(f\"{'R^2':<12} {train_r2:>10.4f} {test_r2:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Interpretation\n",
    "\n",
    "On LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Closed-Form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Closed-form: $\\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "Note: Must prepend a column of ones to X for the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"Prepend a column of ones for the intercept term.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    return np.column_stack([np.ones(N), X])\n",
    "\n",
    "\n",
    "def fit_closed_form(X, y):\n",
    "    \"\"\"Closed-form solution derived in class, generalized to multiple features:\n",
    "    theta = (X^T X)^{-1} X^T y\"\"\"\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"Predict y given X and theta.\"\"\"\n",
    "    return X @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form intercept: 520.414834\n",
      "\n",
      "Closed-form coefficients:\n",
      "      Feature  Coefficient\n",
      "   Unnamed: 0     8.456024\n",
      "     bedrooms   -12.807339\n",
      "    bathrooms    18.456913\n",
      "  sqft_living    57.161582\n",
      "     sqft_lot    11.127338\n",
      "       floors     8.151038\n",
      "   waterfront    64.230911\n",
      "         view    47.610288\n",
      "    condition    12.647609\n",
      "        grade    92.511076\n",
      "   sqft_above    48.439051\n",
      "sqft_basement    27.688812\n",
      "     yr_built   -68.043173\n",
      " yr_renovated    17.341926\n",
      "          lat    78.129852\n",
      "         long    -1.437669\n",
      "sqft_living15    45.479128\n",
      "   sqft_lot15   -12.906560\n",
      "\n",
      "--- Closed-Form Results ---\n",
      "Metric            Train       Test\n",
      "MSE          31415.7479 58834.6740\n",
      "R^2              0.7271     0.6471\n",
      "\n",
      "--- Comparison: sklearn vs Closed-Form ---\n",
      "Metric        sklearn Train   CF Train   sklearn Test    CF Test\n",
      "MSE              31415.7479 31415.7479     58834.6740 58834.6740\n",
      "R^2                  0.7271     0.7271         0.6471     0.6471\n",
      "\n",
      "Max coefficient difference: 0.0000\n",
      "Intercept difference: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Add intercept column to train and test\n",
    "X_train_cf = add_intercept(X_train)\n",
    "X_test_cf = add_intercept(X_test)\n",
    "\n",
    "# Fit closed-form model\n",
    "theta_cf = fit_closed_form(X_train_cf, y_train)\n",
    "\n",
    "# Report coefficients vertically\n",
    "print(f\"Closed-form intercept: {theta_cf[0]:.6f}\\n\")\n",
    "print(\"Closed-form coefficients:\")\n",
    "cf_coeff_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': theta_cf[1:]\n",
    "})\n",
    "print(cf_coeff_df.to_string(index=False))\n",
    "\n",
    "# Training metrics\n",
    "y_train_pred_cf = predict(X_train_cf, theta_cf)\n",
    "cf_train_mse = mean_squared_error(y_train, y_train_pred_cf)\n",
    "cf_train_r2 = r2_score(y_train, y_train_pred_cf)\n",
    "\n",
    "# Testing metrics\n",
    "y_test_pred_cf = predict(X_test_cf, theta_cf)\n",
    "cf_test_mse = mean_squared_error(y_test, y_test_pred_cf)\n",
    "cf_test_r2 = r2_score(y_test, y_test_pred_cf)\n",
    "\n",
    "print(f\"\\n--- Closed-Form Results ---\")\n",
    "print(f\"{'Metric':<12} {'Train':>10} {'Test':>10}\")\n",
    "print(f\"{'MSE':<12} {cf_train_mse:>10.4f} {cf_test_mse:>10.4f}\")\n",
    "print(f\"{'R^2':<12} {cf_train_r2:>10.4f} {cf_test_r2:>10.4f}\")\n",
    "\n",
    "# Comparison with Problem 2\n",
    "print(f\"\\n--- Comparison: sklearn vs Closed-Form ---\")\n",
    "print(f\"{'Metric':<12} {'sklearn Train':>14} {'CF Train':>10} {'sklearn Test':>14} {'CF Test':>10}\")\n",
    "print(f\"{'MSE':<12} {train_mse:>14.4f} {cf_train_mse:>10.4f} {test_mse:>14.4f} {cf_test_mse:>10.4f}\")\n",
    "print(f\"{'R^2':<12} {train_r2:>14.4f} {cf_train_r2:>10.4f} {test_r2:>14.4f} {cf_test_r2:>10.4f}\")\n",
    "\n",
    "# Check coefficient match\n",
    "coeff_diff = np.max(np.abs(lr_model.coef_ - theta_cf[1:]))\n",
    "intercept_diff = np.abs(lr_model.intercept_ - theta_cf[0])\n",
    "print(f\"\\nMax coefficient difference: {coeff_diff:.4f}\")\n",
    "print(f\"Intercept difference: {intercept_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Problem 2\n",
    "\n",
    "On LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "For degree $p$, construct features $[X, X^2, \\ldots, X^p]$, then apply closed-form from Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features [X, X^2, ..., X^degree] from a single feature vector.\"\"\"\n",
    "    return np.column_stack([X**p for p in range(1, degree + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p       Train MSE     Test MSE    Train R^2     Test R^2\n",
      "--------------------------------------------------------\n",
      "1      57947.5262   88575.9785       0.4967       0.4687\n",
      "2      54822.6651   71791.6795       0.5238       0.5694\n",
      "3      53785.1947   99833.4838       0.5329       0.4012\n",
      "4      52795.7748  250979.2743       0.5415      -0.5053\n",
      "5      52626.1120  570616.9148       0.5429      -2.4225\n"
     ]
    }
   ],
   "source": [
    "# Extract sqft_living feature (single feature for polynomial regression)\n",
    "sqft_idx = feature_cols.index('sqft_living')\n",
    "X_train_sqft_raw = X_train_raw[:, sqft_idx].reshape(-1, 1)\n",
    "X_test_sqft_raw = X_test_raw[:, sqft_idx].reshape(-1, 1)\n",
    "\n",
    "# Standardize sqft_living independently\n",
    "sqft_scaler = StandardScaler()\n",
    "X_train_sqft = sqft_scaler.fit_transform(X_train_sqft_raw).flatten()\n",
    "X_test_sqft = sqft_scaler.transform(X_test_sqft_raw).flatten()\n",
    "\n",
    "# Run polynomial regression for p = 1, 2, 3, 4, 5\n",
    "print(f\"{'p':<4} {'Train MSE':>12} {'Test MSE':>12} {'Train R^2':>12} {'Test R^2':>12}\")\n",
    "print(\"-\" * 56)\n",
    "\n",
    "for p in range(1, 6):\n",
    "    # Create polynomial features\n",
    "    X_train_poly = polynomial_features(X_train_sqft, p)\n",
    "    X_test_poly = polynomial_features(X_test_sqft, p)\n",
    "    \n",
    "    # Add intercept\n",
    "    X_train_poly = add_intercept(X_train_poly)\n",
    "    X_test_poly = add_intercept(X_test_poly)\n",
    "    \n",
    "    # Fit using closed-form from Problem 3\n",
    "    theta_poly = fit_closed_form(X_train_poly, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred_poly = predict(X_train_poly, theta_poly)\n",
    "    y_test_pred_poly = predict(X_test_poly, theta_poly)\n",
    "    \n",
    "    # Metrics\n",
    "    poly_train_mse = mean_squared_error(y_train, y_train_pred_poly)\n",
    "    poly_test_mse = mean_squared_error(y_test, y_test_pred_poly)\n",
    "    poly_train_r2 = r2_score(y_train, y_train_pred_poly)\n",
    "    poly_test_r2 = r2_score(y_test, y_test_pred_poly)\n",
    "    \n",
    "    print(f\"{p:<4} {poly_train_mse:>12.4f} {poly_test_mse:>12.4f} {poly_train_r2:>12.4f} {poly_test_r2:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "On LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Implementation\n",
    "\n",
    "Update rule: $\\theta := \\theta - \\alpha \\cdot \\frac{1}{N} X^T(X\\theta - y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, n_iterations, theta_init=None):\n",
    "    \"\"\"\n",
    "    Gradient descent for linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "        X: feature matrix with intercept column (N x (d+1))\n",
    "        y: target vector (N,)\n",
    "        alpha: learning rate\n",
    "        n_iterations: number of iterations\n",
    "        theta_init: initial parameter vector (optional, defaults to zeros)\n",
    "    \n",
    "    Returns:\n",
    "        theta: learned parameters\n",
    "        loss_history: list of MSE at each iteration\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    if theta_init is None:\n",
    "        theta = np.zeros(d)\n",
    "    else:\n",
    "        theta = theta_init.copy()\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Gradient: (1/N) * X^T (X*theta - y)\n",
    "        residuals = X @ theta - y\n",
    "        gradient = (1 / N) * (X.T @ residuals)\n",
    "        \n",
    "        # Update\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        # Track MSE\n",
    "        mse = np.mean(residuals**2)\n",
    "        loss_history.append(mse)\n",
    "    \n",
    "    return theta, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha    iters         Train MSE       Test MSE    Train R^2     Test R^2\n",
      "---------------------------------------------------------------------------\n",
      "0.01     10            294796.69      352448.96      -1.5604      -1.1139\n",
      "0.01     50            138298.52      170450.93      -0.2012      -0.0223\n",
      "0.01     100            70094.38       94751.22       0.3912       0.4317\n",
      "\n",
      "0.05     10            135793.77      167419.60      -0.1794      -0.0042\n",
      "0.05     50             33384.66       58922.60       0.7100       0.6466\n",
      "0.05     100            31514.36       58907.87       0.7263       0.6467\n",
      "\n",
      "0.1      10             66473.61       90873.01       0.4227       0.4550\n",
      "0.1      50             31510.72       58929.63       0.7263       0.6466\n",
      "0.1      100            31427.50       58891.74       0.7270       0.6468\n",
      "\n",
      "--- Closed-form reference ---\n",
      "CF       --             31415.75       58834.67       0.7271       0.6471\n"
     ]
    }
   ],
   "source": [
    "# Problem 5, Part 2: Vary learning rate and iterations\n",
    "alphas = [0.01, 0.05, 0.1]\n",
    "checkpoints = [10, 50, 100]\n",
    "\n",
    "# Use X_train_cf and X_test_cf (with intercept) from Problem 3\n",
    "print(f\"{'alpha':<8} {'iters':<8} {'Train MSE':>14} {'Test MSE':>14} {'Train R^2':>12} {'Test R^2':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for alpha in alphas:\n",
    "    for n_iter in checkpoints:\n",
    "        theta_gd, loss_hist = gradient_descent(X_train_cf, y_train, alpha, n_iter)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred_gd = predict(X_train_cf, theta_gd)\n",
    "        y_test_pred_gd = predict(X_test_cf, theta_gd)\n",
    "        \n",
    "        # Metrics\n",
    "        gd_train_mse = mean_squared_error(y_train, y_train_pred_gd)\n",
    "        gd_test_mse = mean_squared_error(y_test, y_test_pred_gd)\n",
    "        gd_train_r2 = r2_score(y_train, y_train_pred_gd)\n",
    "        gd_test_r2 = r2_score(y_test, y_test_pred_gd)\n",
    "        \n",
    "        print(f\"{alpha:<8} {n_iter:<8} {gd_train_mse:>14.2f} {gd_test_mse:>14.2f} {gd_train_r2:>12.4f} {gd_test_r2:>12.4f}\")\n",
    "    print()\n",
    "\n",
    "# Compare best GD result with closed-form\n",
    "print(\"--- Closed-form reference ---\")\n",
    "print(f\"{'CF':<8} {'--':<8} {cf_train_mse:>14.2f} {cf_test_mse:>14.2f} {cf_train_r2:>12.4f} {cf_test_r2:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Observations\n",
    "\n",
    "On LaTeX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: Ridge Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Ridge regression with gradient descent\n",
    "\n",
    "Modified gradient: $\\nabla J(\\theta) = \\frac{1}{N} X^T(X\\theta - y) + \\frac{2\\lambda}{N} \\theta$\n",
    "\n",
    "Note: Typically do not regularize the intercept term $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_ridge(X, y, alpha, n_iterations, lam, theta_init=None):\n",
    "    \"\"\"\n",
    "    Gradient descent for ridge regression.\n",
    "    \n",
    "    Parameters:\n",
    "        X: feature matrix with intercept column (N x (d+1))\n",
    "        y: target vector (N,)\n",
    "        alpha: learning rate\n",
    "        n_iterations: number of iterations\n",
    "        lam: regularization parameter lambda\n",
    "        theta_init: initial parameter vector (optional, defaults to zeros)\n",
    "    \n",
    "    Returns:\n",
    "        theta: learned parameters\n",
    "        loss_history: list of loss at each iteration\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    if theta_init is None:\n",
    "        theta = np.zeros(d)\n",
    "    else:\n",
    "        theta = theta_init.copy()\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Gradient: (1/N) * X^T(X*theta - y) + (2*lambda/N) * theta\n",
    "        residuals = X @ theta - y\n",
    "        gradient = (1 / N) * (X.T @ residuals)\n",
    "        \n",
    "        # Add regularization term (don't regularize intercept theta[0])\n",
    "        reg_term = (2 * lam / N) * theta\n",
    "        reg_term[0] = 0  # no penalty on intercept\n",
    "        gradient += reg_term\n",
    "        \n",
    "        # Update\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        # Track ridge loss: MSE + lambda * ||theta[1:]||^2\n",
    "        mse = np.mean(residuals**2)\n",
    "        ridge_loss = mse + lam * np.sum(theta[1:]**2)\n",
    "        loss_history.append(ridge_loss)\n",
    "    \n",
    "    return theta, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Simulation study\n",
    "\n",
    "Generate: $X_i \\sim \\text{Uniform}[-2, 2]$, $Y_i = 1 + 2X_i + e_i$, $e_i \\sim N(0, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda          Slope    Intercept          MSE        R^2\n",
      "----------------------------------------------------------\n",
      "None           1.9226       1.1948       3.8999     0.5639\n",
      "1              1.9198       1.1947       3.8999     0.5639\n",
      "10             1.8948       1.1937       3.9009     0.5638\n",
      "100            1.6768       1.1852       3.9823     0.5547\n",
      "1000           0.7796       1.1502       5.6821     0.3646\n",
      "10000          0.1228       1.1245       8.3189     0.0697\n",
      "\n",
      "True parameters: intercept = 1, slope = 2\n"
     ]
    }
   ],
   "source": [
    "# Problem 6, Part 3: Simulation study\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data: X ~ Uniform[-2, 2], Y = 1 + 2X + e, e ~ N(0, 2)\n",
    "N_sim = 1000\n",
    "X_sim = np.random.uniform(-2, 2, N_sim)\n",
    "e_sim = np.random.normal(0, 2, N_sim)\n",
    "y_sim = 1 + 2 * X_sim + e_sim\n",
    "\n",
    "# Prepare feature matrix with intercept\n",
    "X_sim_mat = add_intercept(X_sim.reshape(-1, 1))\n",
    "\n",
    "# Settings\n",
    "lambdas = [0, 1, 10, 100, 1000, 10000]\n",
    "n_iter_sim = 1000\n",
    "\n",
    "print(f\"{'Lambda':<10} {'Slope':>10} {'Intercept':>12} {'MSE':>12} {'R^2':>10}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for lam in lambdas:\n",
    "    # Adaptive learning rate: large lambda makes the gradient huge,\n",
    "    # so we need a smaller step size to avoid divergence.\n",
    "    # The dominant eigenvalue of the Hessian scales with (||X^T X|| + lambda),\n",
    "    # so alpha ~ 1 / (||X^T X|| + lambda) keeps things stable.\n",
    "    alpha_sim = 0.1 / (1 + lam / 1000)\n",
    "    \n",
    "    if lam == 0:\n",
    "        theta_sim, _ = gradient_descent(X_sim_mat, y_sim, alpha_sim, n_iter_sim)\n",
    "    else:\n",
    "        theta_sim, _ = gradient_descent_ridge(X_sim_mat, y_sim, alpha_sim, n_iter_sim, lam)\n",
    "    \n",
    "    y_sim_pred = predict(X_sim_mat, theta_sim)\n",
    "    sim_mse = mean_squared_error(y_sim, y_sim_pred)\n",
    "    sim_r2 = r2_score(y_sim, y_sim_pred)\n",
    "    \n",
    "    label = \"None\" if lam == 0 else str(lam)\n",
    "    print(f\"{label:<10} {theta_sim[1]:>10.4f} {theta_sim[0]:>12.4f} {sim_mse:>12.4f} {sim_r2:>10.4f}\")\n",
    "\n",
    "print(f\"\\nTrue parameters: intercept = 1, slope = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "On LaTeX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS4400 (Python 3.13.1)",
   "language": "python",
   "name": "ds4400"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
